apiVersion: batch/v1
kind: CronJob
metadata:
  name: efh-db-backup
  namespace: efh-prod
spec:
  schedule: '0 4 * * *' # daily at 4am UTC
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
            - name: db-backup
              image: postgres:16-alpine
              envFrom:
                - secretRef:
                    name: efh-db-backup
              command:
                - /bin/sh
                - -c
                - |
                  set -e
                  echo "Starting DB backup..."
                  export PGPASSWORD=$(echo $DATABASE_URL | sed -E 's|.*://([^:]+):([^@]+)@.*|\2|')
                  HOST=$(echo $DATABASE_URL | sed -E 's|.*://([^:]+):([^@]+)@([^:/]+):([0-9]+)/([^?]+).*|\3|')
                  PORT=$(echo $DATABASE_URL | sed -E 's|.*://([^:]+):([^@]+)@([^:/]+):([0-9]+)/([^?]+).*|\4|')
                  USER=$(echo $DATABASE_URL | sed -E 's|.*://([^:]+):([^@]+)@([^:/]+):([0-9]+)/([^?]+).*|\1|')
                  DB=$(echo $DATABASE_URL | sed -E 's|.*://([^:]+):([^@]+)@([^:/]+):([0-9]+)/([^?]+).*|\5|')

                  TIMESTAMP=$(date +%Y%m%d-%H%M%S)
                  FILE="/tmp/${DB}-${TIMESTAMP}.sql.gz"

                  pg_dump -h "$HOST" -p "$PORT" -U "$USER" "$DB" | gzip > "$FILE"

                  echo "Uploading to S3..."
                  apk add --no-cache aws-cli >/dev/null 2>&1 || true
                  aws s3 cp "$FILE" "s3://${S3_BACKUP_BUCKET}/$DB/$DB-$TIMESTAMP.sql.gz"

                  echo "Backup completed: $DB-$TIMESTAMP.sql.gz"
              resources:
                requests:
                  cpu: 100m
                  memory: 256Mi
                limits:
                  cpu: 500m
                  memory: 512Mi
