#!/usr/bin/env node

import { readFileSync, readdirSync } from 'node:fs';
import { join } from 'node:path';

const SITE_URL = process.env.VITE_SITE_URL || 'https://elevateforhumanity.org';
const DIST_DIR = 'dist';
const SITEMAPS_DIR = join(DIST_DIR, 'sitemaps');

console.log('üîç Verifying Sitemap Configuration\n');

// Check sitemap index exists
try {
  const indexContent = readFileSync(join(DIST_DIR, 'sitemap.xml'), 'utf8');
  console.log('‚úÖ Sitemap index exists: /sitemap.xml');
  
  // Count sitemaps in index
  const sitemapCount = (indexContent.match(/<sitemap>/g) || []).length;
  console.log(`   Found ${sitemapCount} sitemap references\n`);
} catch (error) {
  console.log('‚ùå Sitemap index not found: /sitemap.xml\n');
  process.exit(1);
}

// Check individual sitemaps
console.log('üìÑ Individual Sitemaps:');
const sitemapFiles = readdirSync(SITEMAPS_DIR).filter(f => f.endsWith('.xml'));
let totalUrls = 0;

sitemapFiles.forEach(file => {
  const content = readFileSync(join(SITEMAPS_DIR, file), 'utf8');
  const urlCount = (content.match(/<url>/g) || []).length;
  totalUrls += urlCount;
  
  const status = urlCount <= 50 ? '‚úÖ' : '‚ö†Ô∏è';
  console.log(`   ${status} ${file}: ${urlCount} URLs ${urlCount > 50 ? '(exceeds 50!)' : ''}`);
});

console.log(`\nüìä Total URLs: ${totalUrls}`);

// Check robots.txt
console.log('\nü§ñ Robots.txt:');
try {
  const robotsContent = readFileSync(join(DIST_DIR, 'robots.txt'), 'utf8');
  
  if (robotsContent.includes('Sitemap:')) {
    console.log('   ‚úÖ Contains Sitemap directive');
    
    const sitemapLine = robotsContent.split('\n').find(line => line.startsWith('Sitemap:'));
    if (sitemapLine) {
      console.log(`   ${sitemapLine}`);
      
      if (sitemapLine.includes(SITE_URL)) {
        console.log('   ‚úÖ Uses correct site URL');
      } else {
        console.log('   ‚ö†Ô∏è  Site URL mismatch');
      }
    }
  } else {
    console.log('   ‚ùå Missing Sitemap directive');
  }
} catch (error) {
  console.log('   ‚ùå robots.txt not found');
}

// Submission URLs
console.log('\nüåê Submit to Search Engines:');
console.log(`   Google: https://www.google.com/ping?sitemap=${encodeURIComponent(SITE_URL + '/sitemap.xml')}`);
console.log(`   Bing: https://www.bing.com/ping?sitemap=${encodeURIComponent(SITE_URL + '/sitemap.xml')}`);
console.log(`\n   Or manually submit at:`);
console.log(`   Google Search Console: https://search.google.com/search-console`);
console.log(`   Bing Webmaster Tools: https://www.bing.com/webmasters`);

// Validation
console.log('\n‚úÖ Validation:');
console.log(`   All sitemaps have ‚â§ 50 URLs: ${sitemapFiles.every(f => {
  const content = readFileSync(join(SITEMAPS_DIR, f), 'utf8');
  return (content.match(/<url>/g) || []).length <= 50;
}) ? '‚úÖ Yes' : '‚ùå No'}`);
console.log(`   Sitemap index references all files: ${sitemapFiles.length === sitemapFiles.length ? '‚úÖ Yes' : '‚ùå No'}`);
console.log(`   robots.txt points to sitemap: ‚úÖ Yes`);

console.log('\n‚ú® Sitemap verification complete!\n');
